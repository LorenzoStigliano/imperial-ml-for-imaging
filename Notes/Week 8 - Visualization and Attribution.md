- Visualization and attribution: Identify input features responsible for model decision

### How to do this?
- Direct visualization of filters
	- Easy to implement 
	- Limited practical value
	- First layers are easy to interpret (mostly low-level features) 
	- Higher layers are more difficult to interpret (non-interpretable features)
	- Problem: Visualization of filters has limited value 
	- Solution: Instead visualize activations generated by kernels

- Visualize activations generated by kernels
	- Strong response: Feature is present 
	- Weak response: Feature is not present
	- Easy to implement 
	- Easy to interpret for early layers
	- Higher layers are more sparse 
	- Channels may correspond to specific features

- Mask out region in the input image and observe network output
	- If masked out region causes a significant drop in confidence, the masked-out region is important![[Screenshot 2023-02-25 at 12.39.57.png]]

## Saliency maps 
- DeconvNet
	- Given a trained network and an image 
	- Chose activation at one layer (set all others to zero) 
	- Invert network
	- ![[Screenshot 2023-02-25 at 12.43.18.png]]
	- No training involved 
	- Backward pass in network is almost identical to backpropagation (apart from ReLUs)
	- We need to record the positions of the max pooling when doing deconvolution
	- DeconvNet, on the other hand, is a network architecture that is trained to learn a mapping from feature maps in the network's later layers back to the pixel space. During the forward pass, the DeconvNet performs convolutional operations to extract features from the input image. During the backward pass, the DeconvNet performs deconvolutional operations to reconstruct the pixel space from the learned features.
	- To generate saliency maps using DeconvNet, the network is trained to learn the inverse mapping from the output score to the input image. During inference, the DeconvNet can be used to reconstruct the input image from the learned features by performing a backpropagation-like operation. This generates a saliency map that highlights the regions of the input image that are most important for the network's decision.

Question: 
- Which pixels are most significant to a neuron? 
- How would they need to change to most affect the activation of the neuron?
Solution: 
- Use back propagation but differentiate activation with respect to **input pixels**, not weights
- We use the activation function as a loss function and take the derivative using the pixels
- Gradient (backprogagation)
	- Define loss as activation of arbitrary neuron in any layer (last layer is most interesting)![[Screenshot 2023-02-25 at 12.53.50.png]]
- Guided backpropagation
	- Improve results by “guiding” the backpropagation process
	- Idea: 
		- Positive gradients = features the neuron is interested in 
		- Negative gradients = features the neuron is not interested in 
	- Set all negative gradients in the backpropagation to zero
	- Propagating through the ReLu![[Screenshot 2023-02-25 at 12.55.18.png]]
![[Screenshot 2023-02-25 at 12.57.08.png]]
