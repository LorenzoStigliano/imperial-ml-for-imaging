{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Example: PCA with Brain Shapes"
      ],
      "metadata": {
        "id": "QV7buE8ZZMgz"
      },
      "id": "QV7buE8ZZMgz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the data"
      ],
      "metadata": {
        "id": "gQqJEMb1Zn0Z"
      },
      "id": "gQqJEMb1Zn0Z"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "earned-actor"
      },
      "source": [
        "! wget https://www.dropbox.com/s/4xraqjtplz5e8ku/brainshape-data.zip\n",
        "! unzip brainshape-data.zip"
      ],
      "id": "earned-actor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGxMcJQWJ5qu"
      },
      "source": [
        "!pip install vtk"
      ],
      "id": "vGxMcJQWJ5qu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = 'data/brainshapes/'"
      ],
      "metadata": {
        "id": "mqsSK6LxJUX-"
      },
      "id": "mqsSK6LxJUX-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dress-proxy"
      },
      "source": [
        "import numpy as np\n",
        "import vtk\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "id": "dress-proxy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shape registration"
      ],
      "metadata": {
        "id": "bAUz7AgXaVTE"
      },
      "id": "bAUz7AgXaVTE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "third-baltimore"
      },
      "source": [
        "def umeyama_rigid(X, Y):\n",
        "    \n",
        "    # Get dimension and number of points\n",
        "    m, n = X.shape\n",
        "    \n",
        "    # Demean the point sets X and Y\n",
        "    X_mean = X.mean(1)\n",
        "    Y_mean = Y.mean(1)\n",
        "\n",
        "    X_demean =  X - np.tile(X_mean, (n, 1)).T\n",
        "    Y_demean =  Y - np.tile(Y_mean, (n, 1)).T\n",
        "    \n",
        "    # Computing matrix XY' using demeaned point sets\n",
        "    XY = np.dot(X_demean, Y_demean.T)\n",
        "\n",
        "    # Singular value decomposition\n",
        "    U,D,V = np.linalg.svd(XY,full_matrices=True,compute_uv=True)\n",
        "    V=V.T.copy()\n",
        "    \n",
        "    # Determine rotation\n",
        "    R = np.dot( V, U.T)\n",
        "\n",
        "    # Determine translation\n",
        "    t = Y_mean - np.dot(R, X_mean)\n",
        "    \n",
        "    return R,t"
      ],
      "id": "third-baltimore",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "comparable-timber"
      },
      "source": [
        "def umeyama_similarity(X, Y):\n",
        "    \n",
        "    # Get dimension and number of points\n",
        "    m, n = X.shape\n",
        "\n",
        "    # Demean the point sets X and Y\n",
        "    X_mean = X.mean(1) #MODEL ANSWER\n",
        "    Y_mean = Y.mean(1) #MODEL ANSWER\n",
        "    \n",
        "    X_demean =  X - np.tile(X_mean, (n, 1)).T #MODEL ANSWER\n",
        "    Y_demean =  Y - np.tile(Y_mean, (n, 1)).T #MODEL ANSWER\n",
        "\n",
        "    # Computing matrix XY' using demeaned and NORMALISED point sets (divide by the number of points n)\n",
        "    # See Equation (38) in the paper\n",
        "    XY = np.dot(X_demean, Y_demean.T) / n  #MODEL ANSWER\n",
        "\n",
        "    # Determine variances of points X and Y, see Equation (36),(37) in the paper\n",
        "    X_var = np.mean(np.sum(X_demean*X_demean, 0))\n",
        "    Y_var = np.mean(np.sum(Y_demean*Y_demean, 0))\n",
        "\n",
        "    # Singular value decomposition\n",
        "    U,D,V = np.linalg.svd(XY,full_matrices=True,compute_uv=True)\n",
        "    V=V.T.copy()\n",
        "    \n",
        "    # Determine rotation\n",
        "    R = np.dot( V, U.T) #MODEL ANSWER\n",
        "    \n",
        "    # Determine the scaling, see Equation (42) in the paper (assume S to be the identity matrix, so ignore)\n",
        "    c = np.trace(np.diag(D)) / X_var #MODEL ANSWER\n",
        "\n",
        "    # Determine translation, see Equation (41) in the paper\n",
        "    t = Y_mean - c * np.dot(R, X_mean) #MODEL ANSWER\n",
        "\n",
        "    return R,t,c"
      ],
      "id": "comparable-timber",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reflected-business"
      },
      "source": [
        "def read_vtk(filename):\n",
        "    reader = vtk.vtkPolyDataReader()\n",
        "    reader.SetFileName(filename)\n",
        "    reader.Update()\n",
        "    polydata = reader.GetOutput()\n",
        "    vertices = np.array([polydata.GetPoint(i) for i in range(polydata.GetNumberOfPoints())])\n",
        "    return vertices"
      ],
      "id": "reflected-business",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stylish-impact"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "pts = []\n",
        "for path in Path(data_dir).rglob('*BrStem*.vtk'):\n",
        "    v = read_vtk(str(path))\n",
        "    v = np.hstack( (v[:,0], v[:,1], v[:,2]) )\n",
        "    pts.append(v)\n",
        "pts = np.array(pts).transpose()"
      ],
      "id": "stylish-impact",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dressed-bunch"
      },
      "source": [
        "m, n = pts.shape\n",
        "\n",
        "num_nodes = m//3;\n",
        "x_ind = range(num_nodes)\n",
        "y_ind = range(num_nodes,num_nodes*2)\n",
        "z_ind = range(num_nodes*2,num_nodes*3)\n",
        "\n",
        "cx = pts[x_ind,:];\n",
        "cy = pts[y_ind,:];\n",
        "cz = pts[z_ind,:];\n",
        "\n",
        "print('Dimension:\\t' + str(m))\n",
        "print('Samples:\\t' + str(n))"
      ],
      "id": "dressed-bunch",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acquired-fraud"
      },
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "def plot_pts(x,y,z,max_range=None,marker_size=10,figure_size=5):\n",
        "\n",
        "    fig = plt.figure(figsize=(figure_size, figure_size), dpi=100)\n",
        "    ax = fig.add_subplot(projection='3d')\n",
        "    for s in range(x.shape[1]):\n",
        "        ax.scatter(x[:,s], y[:,s], z[:,s], s=marker_size, marker='.')\n",
        "\n",
        "    if max_range == None:\n",
        "        max_range = np.array([x.max()-x.min(), y.max()-y.min(), z.max()-z.min()]).max() / 2.0\n",
        "\n",
        "    mid_x = (x.max()+x.min()) * 0.5\n",
        "    mid_y = (y.max()+y.min()) * 0.5\n",
        "    mid_z = (z.max()+z.min()) * 0.5\n",
        "    ax.set_xlim(mid_x - max_range, mid_x + max_range)\n",
        "    ax.set_ylim(mid_y - max_range, mid_y + max_range)\n",
        "    ax.set_zlim(mid_z - max_range, mid_z + max_range)\n",
        "    ax.view_init(10,45)\n",
        "    ax.grid()"
      ],
      "id": "acquired-fraud",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wicked-tours"
      },
      "source": [
        "# spatial normalisation\n",
        "cx_norm = cx - np.tile(np.mean(cx,axis=0),(num_nodes,1))\n",
        "cy_norm = cy - np.tile(np.mean(cy,axis=0),(num_nodes,1))\n",
        "cz_norm = cz - np.tile(np.mean(cz,axis=0),(num_nodes,1))\n",
        "\n",
        "plot_pts(cx_norm, cy_norm, cz_norm, marker_size=10)"
      ],
      "id": "wicked-tours",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "developing-component"
      },
      "source": [
        "id_source = 0\n",
        "id_target = 3\n",
        "\n",
        "source = np.vstack( (cx[:,id_source], cy[:,id_source], cz[:,id_source]) )\n",
        "target = np.vstack( (cx[:,id_target], cy[:,id_target], cz[:,id_target]) )"
      ],
      "id": "developing-component",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "infrared-radiation"
      },
      "source": [
        "shapes_x = np.vstack((source[0,:], target[0,:])).transpose()\n",
        "shapes_y = np.vstack((source[1,:], target[1,:])).transpose()\n",
        "shapes_z = np.vstack((source[2,:], target[2,:])).transpose()\n",
        "plot_pts( shapes_x, shapes_y, shapes_z, marker_size=10 )"
      ],
      "id": "infrared-radiation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "intended-rebound"
      },
      "source": [
        "R, t = umeyama_rigid(source, target)\n",
        "warped = np.dot(R,source) + np.tile(t, (num_nodes, 1)).transpose()\n",
        "\n",
        "shapes_x = np.vstack((warped[0,:], target[0,:])).transpose()\n",
        "shapes_y = np.vstack((warped[1,:], target[1,:])).transpose()\n",
        "shapes_z = np.vstack((warped[2,:], target[2,:])).transpose()\n",
        "plot_pts( shapes_x, shapes_y, shapes_z, marker_size=10 )"
      ],
      "id": "intended-rebound",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smaller-accessory"
      },
      "source": [
        "# Switch here between the two methods.\n",
        "use_rigid = False\n",
        "\n",
        "id_target = 0\n",
        "\n",
        "target = np.vstack( (cx[:,id_target], cy[:,id_target], cz[:,id_target]) )\n",
        "\n",
        "cx_norm[:,id_target] = target[0,:]\n",
        "cy_norm[:,id_target] = target[1,:]\n",
        "cz_norm[:,id_target] = target[2,:]\n",
        "\n",
        "for i in range(1,n):\n",
        "    source = np.vstack( (cx[:,i], cy[:,i], cz[:,i]) )\n",
        "    \n",
        "    if use_rigid:\n",
        "        R, t = umeyama_rigid(source, target)        \n",
        "        warped = np.dot(R,source) + np.tile(t, (num_nodes, 1)).transpose()\n",
        "    else:\n",
        "        R, t, c = umeyama_similarity(source, target)\n",
        "        warped = c * np.dot(R,source) + np.tile(t, (num_nodes, 1)).transpose()\n",
        "    \n",
        "    cx_norm[:,i] = warped[0,:]\n",
        "    cy_norm[:,i] = warped[1,:]\n",
        "    cz_norm[:,i] = warped[2,:]\n",
        "    \n",
        "plot_pts(cx_norm, cy_norm, cz_norm, marker_size=10)"
      ],
      "id": "smaller-accessory",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "radio-crown"
      },
      "source": [
        "cx_mean = np.mean(cx_norm,axis=1)\n",
        "cy_mean = np.mean(cy_norm,axis=1)\n",
        "cz_mean = np.mean(cz_norm,axis=1)\n",
        "\n",
        "plot_pts(cx_mean.reshape(-1,1), cy_mean.reshape(-1,1), cz_mean.reshape(-1,1), marker_size=10)"
      ],
      "id": "radio-crown",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Principal Component Analysis"
      ],
      "metadata": {
        "id": "W2Jf_cO_ahmw"
      },
      "id": "W2Jf_cO_ahmw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "logical-merchandise"
      },
      "source": [
        "import sklearn.decomposition as decomp\n",
        "\n",
        "X = np.vstack((cx_norm, cy_norm, cz_norm))\n",
        "m, n = X.shape\n",
        "print('Dimension:\\t' + str(m))\n",
        "print('Samples:\\t' + str(n))"
      ],
      "id": "logical-merchandise",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cognitive-mistake"
      },
      "source": [
        "# Create PCA instance\n",
        "pca = decomp.PCA()\n",
        "\n",
        "# Fit the data\n",
        "pca.fit(X.T)\n",
        "\n",
        "# Get the mean from PCA\n",
        "mu_X = pca.mean_\n",
        "\n",
        "# Get principal modes (a.k.a. components) from PCA\n",
        "U = pca.components_.T\n",
        "\n",
        "# Get the eigenvalues from PCA's singular values\n",
        "D = pca.singular_values_**2 / (n - 1)"
      ],
      "id": "cognitive-mistake",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exposed-johns"
      },
      "source": [
        "# Plot retained variance\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "ax.set_xlabel('Mode')\n",
        "ax.set_ylabel('Retained Variance')\n",
        "plt.show()"
      ],
      "id": "exposed-johns",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naughty-mistress"
      },
      "source": [
        "num_modes = 3\n",
        "for i in range(num_modes):\n",
        "\n",
        "    # add and subtract 2 times the standard deviation from the mean\n",
        "    sp = mu_X + U[:,i] * np.sqrt(D[i]) * 3\n",
        "    sn = mu_X - U[:,i] * np.sqrt(D[i]) * 3\n",
        "    \n",
        "    cxx = np.vstack((mu_X[x_ind], sp[x_ind], sn[x_ind])).T\n",
        "    cyy = np.vstack((mu_X[y_ind], sp[y_ind], sn[y_ind])).T\n",
        "    czz = np.vstack((mu_X[z_ind], sp[z_ind], sn[z_ind])).T\n",
        "        \n",
        "    plot_pts(cxx, cyy, czz, marker_size=10)"
      ],
      "id": "naughty-mistress",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "informational-williams"
      },
      "source": [
        "from ipywidgets import interact, fixed\n",
        "\n",
        "def plot_points(mean_shape,modes,s1,s2,s3,s4,s5,s6):\n",
        "    spine = mu_X + U[:,0] * s1 + U[:,1] * s2 + U[:,2] * s3 + U[:,3] * s4 + U[:,4] * s5 + U[:,5] * s6\n",
        "    sx = spine[x_ind].reshape(-1,1)\n",
        "    sy = spine[y_ind].reshape(-1,1)\n",
        "    sz = spine[z_ind].reshape(-1,1)\n",
        "    plot_pts(sx, sy, sz, max_range=30, marker_size=10)\n",
        "\n",
        "def interactive_pca(mu_X,U,D):\n",
        "    interact(plot_points,mean_shape=fixed(mu_X),modes=fixed(U),\n",
        "             **{'s%d' % (i+1): (-np.sqrt(D[i]) * 10, np.sqrt(D[i]) * 10, np.sqrt(D[i])) for i in range(6)});\n",
        "\n",
        "interactive_pca(mu_X,U,D)"
      ],
      "id": "informational-williams",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "freelance-norfolk"
      },
      "source": [],
      "id": "freelance-norfolk",
      "execution_count": null,
      "outputs": []
    }
  ]
}