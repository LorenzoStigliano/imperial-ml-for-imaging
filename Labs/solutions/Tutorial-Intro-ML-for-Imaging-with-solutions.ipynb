{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"N_DDxV2TGQhV"},"source":["# Tutorial: Introduction to Machine Learning for Imaging"]},{"cell_type":"markdown","metadata":{"id":"3QFLJAN0GQha"},"source":["## Downloading the data"]},{"cell_type":"code","metadata":{"id":"Qr6mTh1lGQhb"},"source":["! wget https://www.dropbox.com/s/hstj44z942i60du/supervised-data.zip\n","! unzip supervised-data.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BG39FkD5GQhb"},"source":["## Setting data directory"]},{"cell_type":"code","metadata":{"id":"gg4JhlGjGQhc"},"source":["# data directory\n","# data_dir = '/vol/lab/course/416/data/mnist/' #DoC lab machines\n","data_dir = 'data/mnist/' #local data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G60XpFHeGQhc"},"source":["## Image classification with Python"]},{"cell_type":"markdown","metadata":{"id":"XQWMo9LVGQhc"},"source":["In this tutorial, we will learn basics of image IO and simple processing, and visualisation in Python. \n","If you want to refresh your python basics, please check this [tutorial](http://cs231n.github.io/python-numpy-tutorial/) from the computer vision course at Stanford.\n","\n","By the end of the tutorial, you should be able to:\n","1. Use python, numpy, and run jupyter notebook\n","2. Build a simple binary classifier \n","3. Implement a logistic regression classifier using numpy"]},{"cell_type":"markdown","metadata":{"id":"uYXa-NZvGQhc"},"source":["---\n","## Import and helper functions"]},{"cell_type":"code","metadata":{"id":"dE-8yiLYGQhc"},"source":["# import common libraries\n","import numpy as np\n","\n","# adjust settings to plot nice figures inline\n","%matplotlib inline\n","import matplotlib\n","import matplotlib.pyplot as plt\n","plt.rcParams['axes.labelsize'] = 14\n","plt.rcParams['xtick.labelsize'] = 12\n","plt.rcParams['ytick.labelsize'] = 12"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3xKpBDzcGQhd"},"source":["import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","#########################################################\n","# functions to plot digits\n","#########################################################\n","\n","def plot_digit(data):\n","    image = data.reshape(28, 28)\n","    plt.imshow(image, cmap = matplotlib.cm.gray,\n","               interpolation=\"nearest\")\n","    plt.colorbar()\n","    # plt.axis(\"off\")\n","\n","\n","def plot_digits(data, n_samples_row=10, colormap = matplotlib.cm.gray):\n","    images = [image.reshape(28,28) for image in data]\n","    n_rows = (len(images) - 1) // n_samples_row + 1\n","    # append empty images if the last row is not complete\n","    empty_images = n_rows * n_samples_row - len(data)\n","    images.append(np.zeros((28, 28 * empty_images)))\n","    # draw row by row\n","    images_row = []\n","    for current_row in range(n_rows):\n","        tmp_row_images = images[current_row * n_samples_row : (current_row + 1) * n_samples_row]\n","        images_row.append(np.concatenate(tmp_row_images, axis=1))\n","    # draw all in one image\n","    image = np.concatenate(images_row, axis=0)\n","    plt.figure(figsize=(n_samples_row,n_rows))\n","    plt.imshow(image, cmap = colormap)\n","    plt.colorbar()\n","    # plt.axis(\"off\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ndkk5u6BGQhd"},"source":["---\n","\n","## MNIST digit recognition\n","\n","In a real ML task, data would be available in a database and organised in tables, documents or files. In this tutorial, we will be using the [MNIST dataset](http://yann.lecun.com/exdb/mnist/), small images of digits handwritten by high school students and employees of the US Census Bureau. It consists of a training set of 60,000 examples, and a test set of 10,000 examples. Each image is size-normalized and centered in a fixed-size image 28x28 pixels, and labeled with the digit it represents. It is kind of the *hello world* of machine learning for imaging. You can find more benchmark datasets [here](https://pytorch.org/docs/stable/torchvision/datasets.html)\n"]},{"cell_type":"code","metadata":{"id":"PqoA8hztGQhd"},"source":["import torchvision.datasets as dset\n","\n","# train data\n","train_set = dset.MNIST(root=data_dir, train=True, download=True)\n","train_data = np.array(train_set.data)\n","train_labels = np.array(train_set.targets)\n","\n","# test data\n","test_set = dset.MNIST(root=data_dir, train=False, download=True)\n","test_data = np.array(test_set.data)\n","test_labels = np.array(test_set.targets)\\\n","\n","# print train and test data details\n","print('Train data:')\n","print('shape (images, x,y) = {}'.format(train_data.shape))\n","print('labels = {}'.format(np.unique(train_labels)))\n","\n","print('Test data:')\n","print('shape (images, x,y) = {}'.format(test_data.shape))\n","print('labels = {}'.format(np.unique(test_labels)))\n","\n","\n","# plot sample digits\n","plot_digits(train_set.data[:100])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fatr6TN1GQhd"},"source":["---\n","Here, we will sort our data and fix the random seed to ensure geting same results everytime you run the experiments. Then plot some sampled digits after sorting the data."]},{"cell_type":"code","metadata":{"id":"fTHTlLpcGQhd"},"source":["# we will sort our data and fix the random generator seed to get similar results from different runs\n","np.random.seed(42)\n","\n","# sort dataset\n","def sort_data(data, labels):\n","        sorted_idxs = np.array(sorted([(target, i) for i, target in enumerate(labels)]))[:, 1]\n","        return data[sorted_idxs], labels[sorted_idxs]\n","\n","############################################################################\n","# Q: use the previous function to sort both training and testing data\n","############################################################################\n","train_data, train_labels = sort_data(train_data, train_labels)\n","test_data, test_labels = sort_data(test_data, test_labels)\n","############################################################################\n","\n","# plot sampled images from sorted data\n","# here it samples 20 samples of [0,1], 30 samples of [2,3,4], and 50 samples of [5,6,7,8,9] - 10 samples for each digit\n","example_images = np.r_[train_data[:12000:600], train_data[13000:30600:600], train_data[30600:60000:590]]\n","plot_digits(example_images)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BSt8JuOSGQhe"},"source":["---\n","## Task 1: Simple Binary Classifier\n","\n","Now our data are cleaned and sorted, we will train a simple binary classifier to distinguish between two selected digits. \n","\n","Data usually is divided into three sets for training, validation, and testing. The training data is used to train the model's parameters, while the validation set is used to adjust the model's hyperparameters. Finally, the performance of the trained model is evaluated on the testing data. For this tutorial we will split the data into train and test for simplification. \n","\n","**Task**\n","\n","1. Extract ones and eights from both training and testing data\n","2. Shuffle training data\n","3. Plot number of images versus number of 'white' pixels per image\n","4. Can you predict the label based only on the number of 'white' pixels? What's the training and testing error for such an approach?"]},{"cell_type":"code","metadata":{"id":"QZ3FMihMGQhe"},"source":["############################################################################\n","# Extract sample digits of ones and eights\n","############################################################################\n","\n","def sample_data_digits(data, labels, labels_to_select):\n","    # convert input 3d arrays to 2d arrays\n","    nsamples, nx, ny = data.shape\n","    data_vec = np.reshape(data,(nsamples,nx*ny))\n","   \n","    selected_indexes = np.isin(labels, labels_to_select)\n","    selected_data = data_vec[selected_indexes]\n","    selected_labels = labels[selected_indexes]\n","    \n","    # Convert images from gray to binary by thresholding intensity values\n","    selected_data = 1.0 * (selected_data >= 128)\n","\n","    # convert labels to binary: digit_0=False, digit_1=True\n","    selected_labels = selected_labels==labels_to_select[1]\n","    \n","    # shuffle data\n","    shuffle_index = np.random.permutation(len(selected_labels))\n","    selected_data, selected_labels = selected_data[shuffle_index], selected_labels[shuffle_index]\n","\n","    return selected_data, selected_labels\n","\n","\n","############################################################################\n","# Q: extract ones and eights digits from both training and testing data \n","############################################################################\n","labels_to_select = [1,8]\n","selected_train_data, selected_train_labels = sample_data_digits(train_data,train_labels,labels_to_select)\n","selected_test_data, selected_test_labels = sample_data_digits(test_data,test_labels,labels_to_select)\n","############################################################################\n","\n","# plot sampled digits\n","plot_digits(selected_train_data[0:50])\n","plt.show()\n","plot_digits(selected_train_data[8000:8050])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BvxNqYxZGQhe"},"source":["############################################################################\n","# Q:plot number of images versus number of 'white' foreground pixels \n","# for both 1s and 8s classes.\n","############################################################################\n","\n","sum_selected_train_data_digit_0 = np.squeeze(np.sum(selected_train_data[selected_train_labels==0],axis=1))\n","sum_selected_train_data_digit_1 = np.squeeze(np.sum(selected_train_data[selected_train_labels==1],axis=1))\n","\n","plt.hist(sum_selected_train_data_digit_0, bins=100, range=(0.0, 255.0), fc=[1,0.5,0,0.5])\n","plt.hist(sum_selected_train_data_digit_1, bins=100, range=(0.0, 255.0), fc=[0,0.5,1,0.5])\n","plt.legend(['ones','eights'])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"91f0V6NgGQhf"},"source":["############################################################################\n","# Q: select threshold value to sperate between the two classes\n","############################################################################\n","threshold = 85"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kc3bzdfUGQhf"},"source":["############################################################################\n","# Q: classify digits using a threshold \n","############################################################################\n","sum_selected_train_data = np.squeeze(np.sum(selected_train_data,axis=1))\n","predicted_train_labels = sum_selected_train_data >= threshold\n","\n","sum_selected_test_data = np.squeeze(np.sum(selected_test_data,axis=1))\n","predicted_test_labels = sum_selected_test_data >= threshold"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iOs13v-EGQhf"},"source":["############################################################################\n","# Q: calculate both training and testing accuracy\n","# You should get accuracies around 89-90%\n","############################################################################\n","\n","train_acc = 100.0 * (predicted_train_labels == selected_train_labels).mean()\n","print('Train accuracy = {:.2f}%'.format(train_acc))\n","\n","test_acc = 100.0 * (predicted_test_labels == selected_test_labels).mean()\n","print('Test accuracy = {:.2f}%'.format(test_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cDkZpfohGQhf"},"source":["---\n","**Task**\n","\n","Repeat the previous examples to classify digits 0s and 8s instead of 1s and 8s. Will the threshold binary classifier differentiate between the two categories based on number of 'white' pixels?"]},{"cell_type":"code","metadata":{"id":"aRSad7bLGQhf"},"source":["############################################################################\n","# Q: extract zeros and eights digits from both training and testing data\n","############################################################################\n","labels_to_select = [0,8]\n","selected_train_data, selected_train_labels = sample_data_digits(train_data,train_labels,labels_to_select)\n","selected_test_data, selected_test_labels = sample_data_digits(test_data,test_labels,labels_to_select)\n","\n","############################################################\n","# Q: plot number of images versus number of pixels\n","############################################################\n","\n","sum_selected_train_data_digit_0 = np.squeeze(np.sum(selected_train_data[selected_train_labels==0],axis=1))\n","sum_selected_train_data_digit_1 = np.squeeze(np.sum(selected_train_data[selected_train_labels==1],axis=1))\n","\n","plt.hist(sum_selected_train_data_digit_0, bins=100, range=(0.0, 255.0), fc=[1,0.5,0,0.5])\n","plt.hist(sum_selected_train_data_digit_1, bins=100, range=(0.0, 255.0), fc=[0,0.5,1,0.5])\n","plt.legend(['zeros','eights'])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_lX9HBqyGQhf"},"source":["############################################################################\n","# Q: select threshold value to sperate between the two classes\n","############################################################################\n","threshold = 120\n","\n","############################################################################\n","# Q: classify digits using a threshold \n","############################################################################\n","\n","# Carefully inspect the histogram!\n","sum_selected_train_data = np.squeeze(np.sum(selected_train_data,axis=1))\n","predicted_train_labels = sum_selected_train_data <= threshold # invert decision\n","\n","sum_selected_test_data = np.squeeze(np.sum(selected_test_data,axis=1))\n","predicted_test_labels = sum_selected_test_data <= threshold # invert decision"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hYpGqylVGQhg"},"source":["train_acc = 100.0 * (predicted_train_labels == selected_train_labels).mean()\n","print('Train accuracy = {:.2f}%'.format(train_acc))\n","\n","test_acc = 100.0 * (predicted_test_labels == selected_test_labels).mean()\n","print('Test accuracy = {:.2f}%'.format(test_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BvUOs8TUGQhg"},"source":["---\n","## Task 2: Logistic Regression using Numpy\n","\n","In the previous example, we used a simple threshold to classify each image of a digit using one feature (number of 'white' pixels).\n","\n","Here, we will use a logistic regression model for the same task but using raw pixel information as input features. The logistic regression function is defined as: $h_{\\Theta}(\\mathbf{x}) =  \\frac{1}{1 + \\exp(- \\Theta^{\\top} \\mathbf{x})}$.\n","\n","It's useful to group all training samples into one big matrix $\\mathbf{X}$ of size *(number_samples x number_features)*, and their labels into one vector $\\mathbf{y}$ as in the code below.\n","\n","Training our model is a loop that includes three main steps\n","1. Evaluate the cost function $J(\\Theta)$\n","2. Compute partial derivatives\n","3. Update the model paramteters\n","\n","---\n","\n","**Task**\n","\n","1. Complete the logistic regression class below \n","2. Train a logistic regression model on the data from the previous example\n","3. Compute train and test accuracies, and compare with the previous results\n","4. Plot the trained parameters and comment on the figure"]},{"cell_type":"code","metadata":{"id":"LEF-1UGQGQhg"},"source":["class LogisticRegression:\n","    def __init__(self, lr=0.05, num_iter=1000, add_bias=True, verbose=True):\n","        self.lr = lr\n","        self.verbose = verbose\n","        self.num_iter = num_iter\n","        self.add_bias = add_bias\n","    \n","    def __add_bias(self, X):\n","        bias = np.ones((X.shape[0], 1))\n","        return np.concatenate((bias, X), axis=1)\n","    \n","\n","\n","    def __loss(self, h, y):\n","        ''' computes loss values '''\n","        y = np.array(y,dtype=float)\n","        ############################################################################\n","        # Q: compute the loss \n","        ############################################################################\n","        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n","\n","    \n","    def fit(self, X, y):\n","        ''' \n","        Optimise our model using gradient descent\n","        Arguments:\n","            X input features\n","            y labels from training data\n","            \n","        '''\n","        if self.add_bias:\n","            X = self.__add_bias(X)\n","        \n","        ############################################################################\n","        # Q: initialise weights randomly with normal distribution N(0,0.01)\n","        ############################################################################\n","        self.theta = np.random.normal(0.0,0.01,X.shape[1])\n","        \n","        for i in range(self.num_iter):\n","            ############################################################################\n","            # Q: forward propagation\n","            ############################################################################\n","            z = X.dot(self.theta)\n","            h = 1.0 / (1.0 + np.exp(-z))\n","            ############################################################################\n","            # Q: backward propagation\n","            ############################################################################\n","            gradient = np.dot(X.T, (h - y)) / y.size\n","            # update parameters\n","            self.theta -= self.lr * gradient\n","            ############################################################################\n","            # Q: print loss\n","            ############################################################################\n","            if(self.verbose == True and i % 50 == 0):\n","                h = 1.0 / (1.0 + np.exp(-X.dot(self.theta)))\n","                print('loss: {} \\t'.format(self.__loss(h, y)))\n","    \n","    def predict_probs(self,X):\n","        ''' returns output probabilities\n","        '''\n","        ############################################################################\n","        # Q: forward propagation\n","        ############################################################################\n","        if self.add_bias:\n","            X = self.__add_bias(X)\n","        z = X.dot(self.theta)\n","        return 1.0 / (1.0 + np.exp(-z))\n","\n","    def predict(self, X, threshold=0.5):\n","        ''' returns output classes\n","        '''\n","        return self.predict_probs(X) >= threshold\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_DOuCL3QGQhg"},"source":["#########################################################################\n","# Q: train our model\n","#########################################################################\n","model = LogisticRegression(lr=1e-2, num_iter=1000)\n","model.fit(selected_train_data, selected_train_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mtXWneG_GQhg"},"source":["#########################################################################\n","# Q: Evaluate the trained model - compute train and test accuracies\n","# You should get accuracies around 98-99%\n","#########################################################################\n","train_preds = model.predict(selected_train_data)\n","logistic_train_acc = 100.0 * (train_preds == selected_train_labels).mean()\n","print('Train accuracy = {:.2f}%'.format(logistic_train_acc))\n","\n","test_preds = model.predict(selected_test_data)\n","logistic_test_acc = 100.0 * (test_preds == selected_test_labels).mean()\n","print('Test accuracy = {:.2f}%'.format(logistic_test_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"60dC1x-aGQhg"},"source":["#########################################################################\n","# Plot trained model params (weights) as an image of size (28x28)\n","#########################################################################\n","plt.imshow(model.theta[:-1].reshape(28,28))\n","plt.colorbar()\n","plt.axis('off')\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QIhVhg5jIppw"},"source":["#########################################################################\n","# Plot test data\n","#########################################################################\n","plot_digits(selected_test_data[0:50])\n","plt.axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qxQqU-LYzwEa"},"source":["#########################################################################\n","# Plot test data multiplied by learned template (model weights)\n","#########################################################################\n","pattern = np.tile(model.theta[:-1], (50, 1))\n","plot_digits(np.multiply(selected_test_data[0:50], pattern), colormap = matplotlib.cm.viridis)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U6HVdqdNGQhg"},"source":["---\n","## Task 3: Using explicit features for classification\n","\n","We have now seen how we can build a digit classifier using the raw pixel information as features. In some ML applications, it is possible (or even desired) to hand engineer the feature extraction stage. Here, we are exploring how far we can get with morphometric features extracted for MNIST digits, namely the area, length, thickness, slant, width, height."]},{"cell_type":"code","metadata":{"id":"eywmzzwHGQhh"},"source":["import os\n","import pandas as pd\n","\n","# Reload MNIST\n","train_data = np.array(train_set.data)\n","train_labels = np.array(train_set.targets)\n","\n","# test data\n","test_data = np.array(test_set.data)\n","test_labels = np.array(test_set.targets)\n","\n","# Read the meta data using pandas\n","train_features = pd.read_csv(data_dir + 'train-morpho.csv')\n","test_features = pd.read_csv(data_dir + 't10k-morpho.csv')\n","train_features.head() # show the first five data entries of the training set"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KR3irOV6GQhh"},"source":["plt.scatter(test_features['area'],test_features['length'], marker='.', c=test_labels)\n","plt.grid()\n","plt.xlabel('feature i')\n","plt.ylabel('feature j')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KBdBPHQKGQhh"},"source":["# Reformat the data\n","X_train = np.transpose(np.array([train_features['area'].values,train_features['length'].values,train_features['thickness'].values,train_features['slant'].values,train_features['width'].values,train_features['height'].values]))\n","X_test = np.transpose(np.array([test_features['area'].values,test_features['length'].values,test_features['thickness'].values,test_features['slant'].values,test_features['width'].values,test_features['height'].values]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4B4D5GiZGQhh"},"source":["def sample_data(data, labels, labels_to_select):\n","    selected_indexes = np.isin(labels, labels_to_select)\n","    selected_data = data[selected_indexes]\n","    selected_labels = labels[selected_indexes]\n","\n","    # convert labels to binary: digit_0=False, digit_1=True\n","    selected_labels = selected_labels==labels_to_select[1]\n","    \n","    # shuffle data\n","    shuffle_index = np.random.permutation(len(selected_labels))\n","    selected_data, selected_labels = selected_data[shuffle_index], selected_labels[shuffle_index]\n","\n","    return selected_data, selected_labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jY99-jAsGQhh"},"source":["Similar to above, let's pick data for a simple binary classification between two digits. Let's start with 0s and 8s."]},{"cell_type":"code","metadata":{"id":"L5kuX8VYGQhh"},"source":["labels_to_select = [0,8]\n","selected_train_data, selected_train_labels = sample_data(X_train,train_labels,labels_to_select)\n","selected_test_data, selected_test_labels = sample_data(X_test,test_labels,labels_to_select)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H6intR52GQhh"},"source":["---\n","**Task**\n","\n","This time we use a logistic regression model from [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n","\n","Train the logistic regression on the data and calculate the classification accuracy for both training and testing."]},{"cell_type":"code","metadata":{"id":"hkz2tOgkGQhh"},"source":["from sklearn import linear_model\n","model = linear_model.LogisticRegression(C=1e5, solver='liblinear')\n","model = model.fit(selected_train_data, selected_train_labels)\n","print(model.coef_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vqNwCAmXGQhh"},"source":["#########################################################################\n","train_preds = model.predict(selected_train_data)\n","logistic_train_acc = 100.0 * (train_preds == selected_train_labels).mean()\n","print('Train accuracy = {:.2f}%'.format(logistic_train_acc))\n","\n","test_preds = model.predict(selected_test_data)\n","logistic_test_acc = 100.0 * (test_preds == selected_test_labels).mean()\n","print('Test accuracy = {:.2f}%'.format(logistic_test_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gs69lwv1GQhi"},"source":["See what happens if we remove the feature with the highest coefficient (i.e., thickness)."]},{"cell_type":"code","metadata":{"id":"UfMTxw2CGQhi"},"source":["# Five features, without thickness\n","X_train = np.transpose(np.array([train_features['area'].values,train_features['length'].values,train_features['slant'].values,train_features['width'].values,train_features['height'].values]))\n","X_test = np.transpose(np.array([test_features['area'].values,test_features['length'].values,test_features['slant'].values,test_features['width'].values,test_features['height'].values]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ur5MrZ-iGQhi"},"source":["labels_to_select = [0,8]\n","selected_train_data, selected_train_labels = sample_data(X_train,train_labels,labels_to_select)\n","selected_test_data, selected_test_labels = sample_data(X_test,test_labels,labels_to_select)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JOAV1-_-GQhi"},"source":["model = linear_model.LogisticRegression(C=1e5, solver='liblinear')\n","model = model.fit(selected_train_data, selected_train_labels)\n","print(model.coef_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8XTSJzmeGQhi"},"source":["#########################################################################\n","train_preds = model.predict(selected_train_data)\n","logistic_train_acc = 100.0 * (train_preds == selected_train_labels).mean()\n","print('Train accuracy = {:.2f}%'.format(logistic_train_acc))\n","\n","test_preds = model.predict(selected_test_data)\n","logistic_test_acc = 100.0 * (test_preds == selected_test_labels).mean()\n","print('Test accuracy = {:.2f}%'.format(logistic_test_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RgaQJQDgGQhi"},"source":["See what happens if we use only thickness as a feature."]},{"cell_type":"code","metadata":{"id":"-sq78JXwGQhi"},"source":["X_train = np.transpose(np.array([train_features['thickness'].values]))\n","X_test = np.transpose(np.array([test_features['thickness'].values]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2TjpmfDFGQhi"},"source":["labels_to_select = [0,8]\n","selected_train_data, selected_train_labels = sample_data(X_train,train_labels,labels_to_select)\n","selected_test_data, selected_test_labels = sample_data(X_test,test_labels,labels_to_select)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LcUWLCKFGQhi"},"source":["model = linear_model.LogisticRegression(C=1e5, solver='liblinear')\n","model = model.fit(selected_train_data, selected_train_labels)\n","print(model.coef_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"POjfJ0s3GQhi"},"source":["#########################################################################\n","train_preds = model.predict(selected_train_data)\n","logistic_train_acc = 100.0 * (train_preds == selected_train_labels).mean()\n","print('Train accuracy = {:.2f}%'.format(logistic_train_acc))\n","\n","test_preds = model.predict(selected_test_data)\n","logistic_test_acc = 100.0 * (test_preds == selected_test_labels).mean()\n","print('Test accuracy = {:.2f}%'.format(logistic_test_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UzuMsBLdGQhi"},"source":["---\n","***Bonus (optional)***\n","\n","Extend all of the above to the full 10-class classification problem.\n"]},{"cell_type":"code","metadata":{"id":"0DH_F1VgGQhj"},"source":[],"execution_count":null,"outputs":[]}]}